<!DOCTYPE html>

<html lang="en">
<head>
<meta charset="utf-8"/>
<title>dalle2</title>
<base href="http://adityaramesh.com/" target="_self"/>
<link href="css/reset.css" rel="stylesheet"/>
<link href="css/tufte.css" rel="stylesheet"/>
<link href="css/latex.css" rel="stylesheet"/>
<link href="css/header_footer.css" rel="stylesheet"/>
<link href="css/table.css" rel="stylesheet"/>
<link href="css/tufte_pandoc_compat.css" rel="stylesheet"/>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML-full" type="text/javascript"></script>
</head>
<body>
<header>
<nav>
<a href="about.html">About</a>
<a href="index.html">Posts</a>
</nav>
</header>
<article>
<section class="level1" id="how-dalle-2-works">
<h1>How DALL·E 2 Works</h1>
<p><label class="margin-toggle" for="sidenote-1">⊕</label><input class="margin-toggle" id="sidenote-1" type="checkbox"/><span class="marginnote"><img src="posts/dalle2/images/variations.png"/> Variations on the OpenAI logo generated by DALL·E 2. The original logo is on the top, and the generated variations are on the bottom.</span> <a href="https://openai.com/dall-e-2">DALL·E 2</a> is a system for text-to-image generation developed <a href="https://arxiv.org/abs/2204.06125">by my coauthors and me</a> at <a href="https://openai.com">OpenAI.</a> Given a caption, the system will attempt to generate a novel image from scratch, pixel-by-pixel, that matches the caption. It also has additional capabilities like inpainting, variations, and text diffs. Inpainting allows the user to erase any region of an image with a brush tool and regenerate what was removed so that it is consistent with a given text prompt and the surrounding image context. Variations allows the user to generate new images based on an existing reference image. These new images will all share the same essential content as the original, but differ in the way the details are put together. Finally, text diffs allow the user to apply a “before and after” transformation to an image. This transformation is specified by an initial caption, which describes the “before” state, and a final caption, which describes the “after” state.</p>
<p>The system underlying DALL·E 2, which we call unCLIP, is based on two key technologies: <a href="https://openai.com/blog/clip/">CLIP</a> and <a href="https://arxiv.org/abs/2006.11239">diffusion</a>. As stated in the blog, CLIP is a model that “efficiently learns visual concepts from natural language supervision”. Diffusion is a technique to train a generative model for images by learning to undo the steps of a fixed corruption process. We briefly describe both of these technologies next.</p>
<p>CLIP is trained on a large, diverse collection of image-text pairs, and consists of two neural networks: an image encoder and a text encoder. Each encoder maps its input to a 1,024-dimensional <em>embedding</em> in an abstract concept space that is shared between both modalities. During each step of training, CLIP receives 32,768 images and their corresponding captions. The encoders are trained to match the embedding of each image with the embedding of its corresponding caption.</p>
<p><label class="margin-toggle" for="sidenote-2">⊕</label><input class="margin-toggle" id="sidenote-2" type="checkbox"/><span class="marginnote"><img src="posts/dalle2/images/clip.png"/> Illustration of the contrastive training objective for CLIP.</span> This simple training objective encourages CLIP to learn about all of the features of an image that people are likely to write about online. These features include things like which objects are present, the aesthetic style, the colors and materials that are used, and so on. By contrast, CLIP is typically <em>not</em> incentivized to preserve information about the relative positions of objects, or information about which attributes apply to which objects. This means that CLIP would have a hard time distinguishing between, say, an image of a red cube on top of a blue cube and another image in which the positions of the two objects are reversed. The reason for this is the contrastive nature of the CLIP training objective: CLIP is only incentivized to learn the features of an image that are sufficient to match it up with the correct caption (out of the other 32,767 for the current training step). Unless it receives a counterexample (i.e., a caption that mentions a blue cube on top of a red cube), CLIP will not learn to preserve information about the objects’ relative positions.</p>
<p><label class="margin-toggle" for="sidenote-3">⊕</label><input class="margin-toggle" id="sidenote-3" type="checkbox"/><span class="marginnote"><img src="posts/dalle2/images/diffusion.gif"/> Illustration of the process used to generate a new image with the diffusion model, created by <a href="https://aqnichol.com">Alex Nichol.</a></span> A diffusion model is trained to undo the steps of a fixed corruption process. Each step of the corruption process adds a small amount of gaussian noise to the image, which effectively removes some of the information in it. After the final step, the image becomes indistinguishable from pure noise. The diffusion model is trained to reverse each step of this process, and in doing so learns to put back information into the image that did not previously exist. To generate an image, we start with pure noise and repeatedly apply the model. This gradually makes the image more and more realistic, with the end result being a pristine, noiseless image.</p>
<p>DALL·E 2 generates images in a two-stage process. In the first stage, a model which we call the prior generates the CLIP image embedding for the image from the given caption. In the second stage, a diffusion model which we call unCLIP generates the image itself from this embedding. The second model is called unCLIP because it effectively reverses the mapping learned by the CLIP image encoder. Since it’s trained to “fill in the details” necessary to produce a realistic image from the CLIP embedding, it will learn to model all of the information that CLIP deems irrelevant for its training objective and hence discards.</p>
<p>There’s a few reasons why it’s advantageous to use this two-stage sampling process, and we discuss two of them here.<label class="margin-toggle sidenote-number" for="sidenote-4"></label><input class="margin-toggle" id="sidenote-4" type="checkbox"/><span class="sidenote"><a href="https://arxiv.org/abs/2204.06125">Our paper</a> discusses more advantages of the two-stage sampling process.</span> Firstly, we can explicitly allocate training compute to modeling the high-level semantics that make images meaningful to humans. Images contain a lot of information, most of which is used to describe to fine-grained, imperceptible details. Only a relatively small sliver of this information is responsible for what makes images visually coherent and meaningful to us, and the CLIP image embedding captures much of this. Training a prior model on the CLIP image embedding allows us to focus on modeling these salient characteristics first, before filling in the details necessary to synthesize a realistic image in the second stage.</p>
<p><label class="margin-toggle" for="sidenote-5">⊕</label><input class="margin-toggle" id="sidenote-5" type="checkbox"/><span class="marginnote"><img src="posts/dalle2/images/house.gif"/> Animation of text diff used to transform a Victorian house into a modern one.</span> The second reason is that CLIP’s multimodal embedding space allows us to leverage <a href="https://arxiv.org/abs/1310.4546">word2vec</a>-style arithmetic for image manipulation, a technique that we call <em>text diffs</em>. Let <span class="math inline">\(f_i\)</span> and <span class="math inline">\(f_t\)</span> denote the CLIP image and text encoders, respectively, and suppose that we have an image of a Victorian house contained in a file <code>house.png</code> which we would like to transform into a modern house. To do this, we first compute</p>
<p><span class="math display">\[
\begin{align}
z_{i0} &amp;= f_i(\texttt{house.png}), \\
z_{t0} &amp;= f_t(\textrm{"a photo of a victorian house"}), \\
z_{t1} &amp;= f_t(\textrm{"a photo of a modern house"}), \quad\textrm{and} \\
z_d &amp;= (z_{t1} - z_{t0}) / \|z_{t1} - z_{t0}\|,
\end{align}
\]</span></p>
<p>where <span class="math inline">\(z_d\)</span> is known as the <em>text diff vector.</em> Next, to transform the house, we rotate between the image embedding <span class="math inline">\(z_{i0}\)</span> and the text diff vector <span class="math inline">\(z_d\)</span> using <span class="math inline">\(z_{i1} = \operatorname{slerp}(z_{i0}, z_d, \theta)\)</span>. Finally, we synthesize an image from <span class="math inline">\(z_{i1}\)</span> using unCLIP. The animation shows the trajectory as <span class="math inline">\(\theta\)</span> is varied from 0 (which reconstructs the original image) to 0.50 (which results in a modernized version of the house).</p>
<p><em>Acknowledgments:</em> I’d like to thank <a href="https://twitter.com/AravSrinivas/with_replies">Aravind Srinivas</a> and Justin Mao-Jones for their feedback on this blog. I’d also like to thank to my coauthors <a href="https://prafulladhariwal.com">Prafulla Dhariwal</a>, <a href="https://aqnichol.com">Alex Nichol</a>, <a href="http://caseychu.io">Casey Chu</a>, and <a href="https://twitter.com/markchen90?lang=en">Mark Chen</a>.</p>
</section>

</article>
<footer>
<hr/>
<div class="credits">
<span><a href="http://github.com/adityaramesh/tufte-blog">Tufte-Blog</a> uses
                    <a href="http://pandoc.org">Pandoc</a> with
                    <a href="http://github.com/edwardtufte/tufte-css">Tufte CSS</a> and
                    <a href="http://mathjax.org">MathJax.</a>
</span></div>
</footer>
</body>
</html>